{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79cadff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eba7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 6\n",
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c15422f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './Roses/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      4\u001b[0m         rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m      5\u001b[0m         rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      6\u001b[0m         horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Roses/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msparse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\preprocessing\\image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1564\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1579\u001b[0m ):\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m \n\u001b[0;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './Roses/train'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './Roses/train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=16,\n",
    "        class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55f2337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Black Spot': 0,\n",
       " 'Downy mildew': 1,\n",
       " 'Fresh Leaf': 2,\n",
       " 'Mosaic': 3,\n",
       " 'Powdery mildew': 4,\n",
       " 'Rust': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ad37f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black Spot',\n",
       " 'Downy mildew',\n",
       " 'Fresh Leaf',\n",
       " 'Mosaic',\n",
       " 'Powdery mildew',\n",
       " 'Rust']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d6dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.06830419 0.06830419 0.06830419]\n",
      "  [0.06859357 0.06859357 0.06859357]\n",
      "  [0.06888296 0.06888296 0.06888296]\n",
      "  ...\n",
      "  [0.04416027 0.09926115 0.06624041]\n",
      "  [0.0594594  0.13251191 0.09165337]\n",
      "  [0.06015893 0.14414938 0.09709056]]\n",
      "\n",
      " [[0.07906725 0.07906725 0.07906725]\n",
      "  [0.07848846 0.07848846 0.07848846]\n",
      "  [0.07790968 0.07790968 0.07790968]\n",
      "  ...\n",
      "  [0.04502844 0.10085279 0.06754266]\n",
      "  [0.05888062 0.13236722 0.09121929]\n",
      "  [0.06247405 0.14675389 0.09969507]]\n",
      "\n",
      " [[0.076158   0.076158   0.076158  ]\n",
      "  [0.07644739 0.07644739 0.07644739]\n",
      "  [0.07673679 0.07673679 0.07673679]\n",
      "  ...\n",
      "  [0.04589661 0.10244444 0.06884491]\n",
      "  [0.05830184 0.13222253 0.0907852 ]\n",
      "  [0.06478917 0.1493584  0.10229958]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.4083542  0.49855027 0.33384436]\n",
      "  [0.38705707 0.48287466 0.3181688 ]\n",
      "  [0.38881645 0.48685566 0.32214975]\n",
      "  ...\n",
      "  [0.4694748  0.57424366 0.47620443]\n",
      "  [0.46961948 0.57453305 0.4764938 ]\n",
      "  [0.46976417 0.5748224  0.47678322]]\n",
      "\n",
      " [[0.4077754  0.49797148 0.33326557]\n",
      "  [0.3861889  0.48229587 0.31759   ]\n",
      "  [0.38925052 0.48728973 0.32258382]\n",
      "  ...\n",
      "  [0.47444242 0.5764032  0.478364  ]\n",
      "  [0.47342953 0.57539034 0.4773511 ]\n",
      "  [0.4724167  0.5743775  0.47633827]]\n",
      "\n",
      " [[0.4071966  0.49739268 0.3326868 ]\n",
      "  [0.38532075 0.4817171  0.3170112 ]\n",
      "  [0.38968462 0.48772383 0.32301793]\n",
      "  ...\n",
      "  [0.5151725  0.61934954 0.5213103 ]\n",
      "  [0.5124233  0.61631095 0.51827174]\n",
      "  [0.5096741  0.61327237 0.51523316]]]\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for image_batch, label_batch in train_generator:\n",
    "#     print(label_batch)\n",
    "    print(image_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b5e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 682 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'Roses/val',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=16,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f5efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 684 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'Roses/test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=16,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82df97fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.39724332 0.47821793 0.4429238 ]\n",
      "  [0.39708206 0.47797602 0.4426819 ]\n",
      "  [0.3969208  0.47773415 0.44244   ]\n",
      "  ...\n",
      "  [0.5453165  0.5886722  0.6225863 ]\n",
      "  [0.5529412  0.6053008  0.6380525 ]\n",
      "  [0.5515629  0.6119782  0.6486507 ]]\n",
      "\n",
      " [[0.38474566 0.4670986  0.43180448]\n",
      "  [0.38522947 0.4675824  0.4322883 ]\n",
      "  [0.38571325 0.4680662  0.43277207]\n",
      "  ...\n",
      "  [0.5455584  0.58915603 0.6229895 ]\n",
      "  [0.5529412  0.6053814  0.6382138 ]\n",
      "  [0.5514822  0.6122201  0.6489732 ]]\n",
      "\n",
      " [[0.38155374 0.46898982 0.43115413]\n",
      "  [0.38139248 0.4686673  0.43091223]\n",
      "  [0.3812312  0.46834478 0.43067035]\n",
      "  ...\n",
      "  [0.5458003  0.5896398  0.62339264]\n",
      "  [0.5529412  0.605462   0.63837504]\n",
      "  [0.5514016  0.612462   0.64929575]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.06428471 0.06736393 0.06582432]\n",
      "  [0.06750736 0.06750736 0.06750736]\n",
      "  [0.06820793 0.06344733 0.06582764]\n",
      "  ...\n",
      "  [0.0579845  0.06190607 0.04538076]\n",
      "  [0.05782324 0.06174481 0.04505823]\n",
      "  [0.05766197 0.06158354 0.0447357 ]]\n",
      "\n",
      " [[0.06420408 0.06712203 0.06566305]\n",
      "  [0.06766862 0.06766862 0.06766862]\n",
      "  [0.0681273  0.06320544 0.06566638]\n",
      "  ...\n",
      "  [0.06582598 0.07128799 0.06260416]\n",
      "  [0.06566472 0.07104609 0.062201  ]\n",
      "  [0.06550345 0.07080419 0.06179783]]\n",
      "\n",
      " [[0.06412345 0.06688014 0.06550179]\n",
      "  [0.06782989 0.06782989 0.06782989]\n",
      "  [0.06804667 0.06296355 0.06550511]\n",
      "  ...\n",
      "  [0.06750902 0.07381255 0.07450981]\n",
      "  [0.06767029 0.07405444 0.07450981]\n",
      "  [0.06783155 0.07429634 0.07450981]]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in test_generator:\n",
    "    print(image_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f324caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING THE MODEL \n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13cd81a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,942\n",
      "Trainable params: 183,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6823b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf0a8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341.25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5460/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82a0d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "682/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6266ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "341/341 [==============================] - 237s 693ms/step - loss: 1.0655 - accuracy: 0.5542 - val_loss: 0.9604 - val_accuracy: 0.6443\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 214s 626ms/step - loss: 0.7073 - accuracy: 0.7019 - val_loss: 0.5647 - val_accuracy: 0.7574\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 213s 626ms/step - loss: 0.5973 - accuracy: 0.7482 - val_loss: 0.6726 - val_accuracy: 0.7202\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 212s 621ms/step - loss: 0.5402 - accuracy: 0.7803 - val_loss: 0.5318 - val_accuracy: 0.7753\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 211s 619ms/step - loss: 0.4582 - accuracy: 0.8216 - val_loss: 0.4196 - val_accuracy: 0.8482\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 212s 621ms/step - loss: 0.4076 - accuracy: 0.8519 - val_loss: 0.3552 - val_accuracy: 0.8765\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 212s 623ms/step - loss: 0.3599 - accuracy: 0.8701 - val_loss: 0.3636 - val_accuracy: 0.8750\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 213s 626ms/step - loss: 0.3044 - accuracy: 0.8909 - val_loss: 0.3820 - val_accuracy: 0.8690\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 213s 624ms/step - loss: 0.2760 - accuracy: 0.9028 - val_loss: 0.2322 - val_accuracy: 0.9196\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 213s 624ms/step - loss: 0.2238 - accuracy: 0.9205 - val_loss: 0.2315 - val_accuracy: 0.9211\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 212s 623ms/step - loss: 0.2164 - accuracy: 0.9269 - val_loss: 0.2299 - val_accuracy: 0.9241\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 262s 769ms/step - loss: 0.2123 - accuracy: 0.9238 - val_loss: 0.1800 - val_accuracy: 0.9420\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 441s 1s/step - loss: 0.1845 - accuracy: 0.9330 - val_loss: 0.1998 - val_accuracy: 0.9345\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 218s 639ms/step - loss: 0.1760 - accuracy: 0.9350 - val_loss: 0.1726 - val_accuracy: 0.9375\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 1039s 3s/step - loss: 0.1840 - accuracy: 0.9372 - val_loss: 0.1990 - val_accuracy: 0.9315\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 221s 647ms/step - loss: 0.1739 - accuracy: 0.9414 - val_loss: 0.3807 - val_accuracy: 0.8973\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 220s 644ms/step - loss: 0.1642 - accuracy: 0.9420 - val_loss: 0.1827 - val_accuracy: 0.9330\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 219s 642ms/step - loss: 0.1408 - accuracy: 0.9528 - val_loss: 0.1628 - val_accuracy: 0.9435\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 219s 643ms/step - loss: 0.1280 - accuracy: 0.9565 - val_loss: 0.1625 - val_accuracy: 0.9524\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 218s 640ms/step - loss: 0.1064 - accuracy: 0.9651 - val_loss: 0.1449 - val_accuracy: 0.9494\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=341,\n",
    "#     batch_size=16,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=42,\n",
    "#     verbose=1,\n",
    "#     epochs=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46386a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('rose_leaf_detection.h5')\n",
    "\n",
    "# load the saved model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "model = keras.models.load_model('rose_leaf_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1645ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 18s 399ms/step - loss: 0.1819 - accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe2119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18188059329986572, 0.9385964870452881]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86687315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#function for inference\n",
      "def predict(model, img):\n",
      "    img_array = tf.keras.preprocessing.image.img_to_array(images[i])\n",
      "    img_array = tf.expand_dims(img_array, 0)\n",
      "\n",
      "    predictions = model.predict(img_array)\n",
      "\n",
      "    predicted_class = class_names[np.argmax(predictions[0])]\n",
      "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
      "    return predicted_class, confidence\n",
      "#function for inference\n",
      "def predict(model, img):\n",
      "    img_array = tf.keras.preprocessing.image.img_to_array(images[i])\n",
      "    img_array = tf.expand_dims(img_array, 0)\n",
      "\n",
      "    predictions = model.predict(img_array)\n",
      "\n",
      "    predicted_class = class_names[np.argmax(predictions[0])]\n",
      "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
      "    return predicted_class, confidence\n",
      "#run inference on few sample images\n",
      "plt.figure(figsize=(15, 15))\n",
      "for images, labels in test_generator:\n",
      "    for i in range(9):\n",
      "        ax = plt.subplot(3, 3, i + 1)\n",
      "        plt.imshow(images[i])\n",
      "        \n",
      "        predicted_class, confidence = predict(model, images[i])\n",
      "        actual_class = class_names[int(labels[i])] \n",
      "        \n",
      "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
      "        \n",
      "        plt.axis(\"off\")\n",
      "    break\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import models, layers\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.display import HTML\n",
      "IMAGE_SIZE = 256\n",
      "CHANNELS = 6\n",
      "BATCH_SIZE=16\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import models, layers\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.display import HTML\n",
      "IMAGE_SIZE = 256\n",
      "CHANNELS = 6\n",
      "BATCH_SIZE=16\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "\n",
      "train_datagen = ImageDataGenerator(\n",
      "        rescale=1./255,\n",
      "        rotation_range=10,\n",
      "        horizontal_flip=True\n",
      ")\n",
      "train_generator = train_datagen.flow_from_directory(\n",
      "        './Roses/train',\n",
      "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
      "        batch_size=16,\n",
      "        class_mode=\"sparse\",\n",
      ")\n",
      "train_generator.class_indices\n",
      "class_names = list(train_generator.class_indices.keys())\n",
      "class_names\n",
      "count=0\n",
      "for image_batch, label_batch in train_generator:\n",
      "#     print(label_batch)\n",
      "    print(image_batch[0])\n",
      "    break\n",
      "validation_datagen = ImageDataGenerator(\n",
      "        rescale=1./255,\n",
      "        rotation_range=10,\n",
      "        horizontal_flip=True)\n",
      "validation_generator = validation_datagen.flow_from_directory(\n",
      "        'Roses/val',\n",
      "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
      "        batch_size=16,\n",
      "        class_mode=\"sparse\"\n",
      ")\n",
      "test_datagen = ImageDataGenerator(\n",
      "        rescale=1./255,\n",
      "        rotation_range=10,\n",
      "        horizontal_flip=True)\n",
      "\n",
      "test_generator = test_datagen.flow_from_directory(\n",
      "        'Roses/test',\n",
      "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
      "        batch_size=16,\n",
      "        class_mode=\"sparse\"\n",
      ")\n",
      "for image_batch, label_batch in test_generator:\n",
      "    print(image_batch[0])\n",
      "    break\n",
      "#BUILDING THE MODEL \n",
      "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
      "n_classes = 6\n",
      "\n",
      "model = models.Sequential([\n",
      "    layers.InputLayer(input_shape=input_shape),\n",
      "    layers.Conv2D(32, kernel_size = (3,3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
      "    layers.MaxPooling2D((2, 2)),\n",
      "    layers.Flatten(),\n",
      "    layers.Dense(64, activation='relu'),\n",
      "    layers.Dense(n_classes, activation='softmax'),\n",
      "])\n",
      "model.summary()\n",
      "model.compile(\n",
      "    optimizer='adam',\n",
      "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
      "    metrics=['accuracy']\n",
      ")\n",
      "#model.save('rose_leaf_detection.h5')\n",
      "# load the saved model\n",
      "import numpy as np\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras.preprocessing import image\n",
      "\n",
      "model = keras.models.load_model('rose_leaf_detection_model.h5')\n",
      "scores = model.evaluate(test_generator)\n",
      "scores\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99618a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e613403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ebbca1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c34f194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0654813051223755,\n",
       " 0.7073193192481995,\n",
       " 0.5973451733589172,\n",
       " 0.5401831269264221,\n",
       " 0.4582422971725464]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss'][:5] # show loss for first 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f46f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e173a7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Run prediction on a sample image\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_generator\u001b[49m:\n\u001b[0;32m      5\u001b[0m     first_image \u001b[38;5;241m=\u001b[39m image_batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m     first_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(label_batch[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_generator' is not defined"
     ]
    }
   ],
   "source": [
    "#Run prediction on a sample image\n",
    "import numpy as np\n",
    "\n",
    "for image_batch, label_batch in test_generator:\n",
    "    first_image = image_batch[0]\n",
    "    first_label = int(label_batch[0])\n",
    "    \n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "    \n",
    "    batch_prediction = model.predict(image_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd214d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for inference\n",
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i])\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f1d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run inference on few sample images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_generator:\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i])\n",
    "        actual_class = class_names[int(labels[i])] \n",
    "       \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00203614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,942\n",
      "Trainable params: 183,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('rose_leaf_detection_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49690a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_image,test_labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5024da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[0;32m     30\u001b[0m     ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     33\u001b[0m     predicted_class, confidence \u001b[38;5;241m=\u001b[39m predict(model, images[i])\n\u001b[0;32m     34\u001b[0m     actual_class \u001b[38;5;241m=\u001b[39m class_names[\u001b[38;5;28mint\u001b[39m(labels[i])] \n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 6\n",
    "BATCH_SIZE=16\n",
    "\n",
    "\n",
    "model = keras.models.load_model('rose_leaf_detection_model.h5')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        './Test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=16,\n",
    "        class_mode=\"sparse\"\n",
    ")\n",
    "\n",
    "#run inference on few sample images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_generator:\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i])\n",
    "        actual_class = class_names[int(labels[i])] \n",
    "       \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
